
#!/usr/bin/env python3
"""
QUANTUM TEMPORAL COHERENCE DISCOVERY EXPERIMENT (ROBUST VERSION)
================================================================
CRITICAL TEST: Particle Lifetime vs Optimal Gaussian Width
HYPOTHESIS: œÉ_optimal ‚àù -log(lifetime)

Features:
- Job timeout handling
- Status monitoring
- Resume capability if interrupted
"""

import numpy as np
from scipy.stats import pearsonr
import time
import json
import pandas as pd
import os

from qiskit import QuantumCircuit, transpile

try:
    from azure.quantum import Workspace
    from azure.quantum.qiskit import AzureQuantumProvider
    AZURE_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  pip install azure-quantum qiskit")
    AZURE_AVAILABLE = False

# ================================================================================
# CONFIGURATION
# ================================================================================

CONNECTION_STRING = "xxxxx"

NUM_QUBITS = 4
NUM_SHOTS = 500
CRITICAL_NOISE = 0.21
JOB_TIMEOUT = 120  # 2 minutes max per job

# Enhanced particle set with known lifetimes
TEMPORAL_TEST_PARTICLES = {
    'electron': {
        'mass': 0.511e6, 'charge': -1, 'spin': 0.5, 'type': 'lepton', 'gen': 1,
        'lifetime': np.inf, 'decay_mode': 'stable'
    },
    'muon': {
        'mass': 105.66e6, 'charge': -1, 'spin': 0.5, 'type': 'lepton', 'gen': 2, 
        'lifetime': 2.196e-6, 'decay_mode': 'e ŒΩ_e ŒΩ_Œº'
    },
    'tau': {
        'mass': 1.777e9, 'charge': -1, 'spin': 0.5, 'type': 'lepton', 'gen': 3,
        'lifetime': 2.903e-13, 'decay_mode': 'multiple_hadronic'
    },
    'W_boson': {
        'mass': 80.379e9, 'charge': 1, 'spin': 1, 'type': 'gauge_boson', 'gen': 0,
        'lifetime': 3.0e-25, 'decay_mode': 'q qÃÑ, l ŒΩ_l'
    }
}

FOCUSED_WIDTHS = [0.3, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]

# Resume capability
CHECKPOINT_FILE = 'temporal_coherence_checkpoint.json'

print("="*80)
print("üî¨ QUANTUM TEMPORAL COHERENCE DISCOVERY EXPERIMENT (ROBUST)")
print("="*80)
print(f"Particles: {len(TEMPORAL_TEST_PARTICLES)}")
print(f"Widths per particle: {len(FOCUSED_WIDTHS)}")
print(f"Job timeout: {JOB_TIMEOUT}s")
print("="*80 + "\n")

# ================================================================================
# GAUSSIAN WIDTH SENSOR
# ================================================================================

class GaussianWidthSensor:
    def __init__(self, seed=None):
        from numpy.random import Generator, Philox
        self.seed = seed or int(time.time() * 1e9) % (2**32)
        self.philox = Generator(Philox(self.seed))
    
    def get_philox_gauss_samples(self, width, n=500):
        return [self.philox.normal(0, width) for _ in range(n)]

sensor = GaussianWidthSensor()
print("‚úì Gaussian width sensor initialized\n")

# ================================================================================
# QUANTUM CIRCUITS
# ================================================================================

def create_particle_circuit(particle_name, particle_data, n_qubits=4, noise=0.0):
    qc = QuantumCircuit(n_qubits, n_qubits)
    
    max_mass = 173e9
    mass_angle = (particle_data['mass'] / max_mass) * np.pi
    charge_angle = (particle_data['charge'] + 1) * np.pi / 2
    spin_angle = particle_data['spin'] * np.pi
    gen = particle_data.get('gen', 0)
    ptype = particle_data['type']
    
    for i in range(n_qubits):
        qc.h(i)
    
    qc.ry(mass_angle, 0)
    qc.ry(charge_angle, 1)
    qc.ry(spin_angle, 2)
    if gen > 0:
        qc.ry(gen * np.pi / 3, 3)
    
    for i in range(n_qubits - 1):
        qc.cx(i, i + 1)
    
    if ptype in ['lepton', 'quark']:
        qc.rz(mass_angle * 0.5, 0)
        qc.rz(charge_angle * 0.5, 1)
        qc.cx(0, 2)
    elif ptype == 'gauge_boson':
        qc.rz(mass_angle * 0.3, 0)
        qc.cx(0, 3)
        qc.cz(1, 3)
    elif ptype == 'scalar':
        qc.rz(mass_angle * 0.3, 0)
        qc.cx(0, 3)
    
    qc.cx(n_qubits - 1, 0)
    
    if noise > 0:
        for i in range(n_qubits):
            angle = noise * np.pi / 2
            qc.rz(angle, i)
            qc.rx(angle * 0.5, i)
    
    for i in range(0, n_qubits, 2):
        if i + 1 < n_qubits:
            qc.ry(np.pi / 8, i)
            qc.ry(np.pi / 8, i + 1)
    
    qc.barrier()
    qc.measure(range(n_qubits), range(n_qubits))
    return qc

print("‚úì Quantum circuit generator ready\n")

# ================================================================================
# ANALYSIS FUNCTIONS
# ================================================================================

def calculate_entropy_discrete(data, n_bins):
    data_norm = np.array(data)
    data_norm = (data_norm - data_norm.min()) / (data_norm.max() - data_norm.min() + 1e-10)
    bins = np.linspace(0, 1, n_bins + 1)
    digitized = np.digitize(data_norm, bins) - 1
    digitized = np.clip(digitized, 0, n_bins - 1)
    counts = np.bincount(digitized, minlength=n_bins)
    probs = counts / len(data)
    probs = probs[probs > 0]
    return -np.sum(probs * np.log2(probs))

def quantum_entropy(counts, n_qubits):
    total = sum(counts.values())
    probs = np.array([counts.get(format(i, f'0{n_qubits}b'), 0) / total 
                      for i in range(2**n_qubits)])
    probs = probs[probs > 0]
    return -np.sum(probs * np.log2(probs))

def coupling_coefficient(classical_entropy, quantum_entropy):
    return quantum_entropy / (classical_entropy + 1e-10)

print("‚úì Analysis functions ready\n")

# ================================================================================
# BACKEND SETUP
# ================================================================================

def setup_backend():
    if not AZURE_AVAILABLE:
        print("‚ùå Azure Quantum SDK not installed")
        return None, None
    
    try:
        workspace = Workspace.from_connection_string(CONNECTION_STRING)
        print(f"‚úì Connected to workspace: {workspace.name}")
        
        provider = AzureQuantumProvider(workspace)
        backends = provider.backends()
        print("\nAvailable backends:")
        for b in backends:
            print(f"  - {b.name()}")
        
        target = 'rigetti.sim.qvm'
        backend = provider.get_backend(target)
        if backend is None:
            print(f"‚ùå {target} not available")
            return None, None
        
        print(f"\n‚úì Using: {backend.name()}\n")
        return backend, backend.name()
                
    except Exception as e:
        print(f"‚ùå Connection failed: {e}")
        return None, None

backend, backend_name = setup_backend()
if backend is None:
    print("Cannot proceed without backend")
    exit(1)

# ================================================================================
# ROBUST JOB EXECUTION
# ================================================================================

def execute_circuit_with_timeout(backend, qc_trans, particle_name, timeout=JOB_TIMEOUT):
    """Execute circuit with timeout and status monitoring"""
    print(f"   Submitting circuit...", end=" ", flush=True)
    
    try:
        job = backend.run(qc_trans, shots=NUM_SHOTS)
        job_id = job.job_id() if hasattr(job, 'job_id') else 'N/A'
        print(f"Job ID: {job_id}")
        print(f"   ‚è≥ Waiting", end="", flush=True)
        
        start_time = time.time()
        
        while True:
            elapsed = time.time() - start_time
            
            if elapsed > timeout:
                print(f"\n   ‚ö†Ô∏è  TIMEOUT after {timeout}s")
                return None
            
            try:
                status = job.status()
                
                if status.name == 'DONE':
                    result = job.result()
                    print(" ‚úì")
                    return result
                elif status.name in ['ERROR', 'CANCELLED']:
                    print(f"\n   ‚ùå Job {status.name}")
                    return None
                
            except Exception as e:
                # Job may not support status(), try direct result
                try:
                    result = job.result()
                    print(" ‚úì")
                    return result
                except:
                    pass
            
            print(".", end="", flush=True)
            time.sleep(2)
            
    except Exception as e:
        print(f"\n   ‚ùå Error: {e}")
        return None

# ================================================================================
# CHECKPOINT SYSTEM
# ================================================================================

def load_checkpoint():
    """Load previous results if available"""
    if os.path.exists(CHECKPOINT_FILE):
        try:
            with open(CHECKPOINT_FILE, 'r') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_checkpoint(results):
    """Save current results"""
    # Convert numpy types
    def convert_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_types(item) for item in obj]
        return obj
    
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(convert_types(results), f, indent=2)

# ================================================================================
# TEMPORAL COHERENCE EXPERIMENT
# ================================================================================

def run_temporal_coherence_experiment():
    print("="*80)
    print("üî¨ QUANTUM TEMPORAL COHERENCE EXPERIMENT")
    print("="*80)
    print("HYPOTHESIS: œÉ_optimal ‚àù -log(lifetime)")
    print("-"*80 + "\n")
    
    results = load_checkpoint()
    n_bins = 2**NUM_QUBITS
    
    for particle_name, particle_data in TEMPORAL_TEST_PARTICLES.items():
        
        # Skip if already completed
        if particle_name in results and 'optimal_width' in results[particle_name]:
            print(f"‚öõÔ∏è  {particle_name.upper()} - SKIPPING (already completed)")
            continue
        
        print(f"‚öõÔ∏è  {particle_name.upper()}")
        print(f"   Mass: {particle_data['mass']:.2e} eV")
        print(f"   Lifetime: {particle_data['lifetime']:.2e} s")
        print(f"   Decay: {particle_data['decay_mode']}")
        
        # Create and transpile circuit
        qc = create_particle_circuit(particle_name, particle_data, NUM_QUBITS, CRITICAL_NOISE)
        qc_trans = transpile(qc, backend=backend, optimization_level=2)
        
        # Execute with timeout
        result = execute_circuit_with_timeout(backend, qc_trans, particle_name)
        
        if result is None:
            print(f"   ‚ö†Ô∏è  Skipping {particle_name} due to timeout/error\n")
            continue
        
        counts = result.get_counts()
        q_entropy = quantum_entropy(counts, NUM_QUBITS)
        print(f"   Quantum entropy: {q_entropy:.4f} bits\n")
        
        # Test each Gaussian width
        width_couplings = {}
        print("   Testing Gaussian widths:")
        
        for width in FOCUSED_WIDTHS:
            classical_samples = sensor.get_philox_gauss_samples(width, NUM_SHOTS)
            c_entropy = calculate_entropy_discrete(classical_samples, n_bins)
            coupling = coupling_coefficient(c_entropy, q_entropy)
            
            width_couplings[width] = {
                'classical_entropy': float(c_entropy),
                'coupling': float(coupling)
            }
            
            print(f"     œÉ={width:.1f}: coupling={coupling:.4f}")
        
        # Find optimal width
        optimal_width = max(width_couplings.keys(), key=lambda w: width_couplings[w]['coupling'])
        optimal_coupling = width_couplings[optimal_width]['coupling']
        
        print(f"\n   üéØ OPTIMAL: œÉ={optimal_width:.1f}, coupling={optimal_coupling:.4f}\n")
        
        results[particle_name] = {
            'lifetime': float(particle_data['lifetime']) if particle_data['lifetime'] != np.inf else 'inf',
            'mass': float(particle_data['mass']),
            'type': particle_data['type'],
            'optimal_width': float(optimal_width),
            'optimal_coupling': float(optimal_coupling),
            'quantum_entropy': float(q_entropy),
            'width_couplings': width_couplings
        }
        
        # Save checkpoint after each particle
        save_checkpoint(results)
        print(f"   ‚úì Checkpoint saved\n")
    
    return results

def analyze_temporal_structure(results):
    print("="*80)
    print("üìä TEMPORAL COHERENCE ANALYSIS")
    print("="*80 + "\n")
    
    particles = list(results.keys())
    lifetimes = [results[p]['lifetime'] if results[p]['lifetime'] != 'inf' else np.inf 
                 for p in particles]
    optimal_widths = [results[p]['optimal_width'] for p in particles]
    
    # Handle infinite lifetime
    finite_lifetimes = [lt if lt != np.inf else 1e20 for lt in lifetimes]
    log_lifetimes = np.log10(finite_lifetimes)
    
    # Correlation
    correlation, p_value = pearsonr(log_lifetimes, optimal_widths)
    
    print(f"CORRELATION ANALYSIS:")
    print(f"  r = {correlation:.3f}, p = {p_value:.4f}\n")
    
    # Predictions
    predictions = {
        'electron': 1.0,
        'muon': 2.0,
        'tau': 2.8,
        'W_boson': 0.4
    }
    
    print(f"{'Particle':<15} {'Predicted':<12} {'Observed':<12} {'Error':<10}")
    print("-"*50)
    
    total_error = 0
    for particle in particles:
        pred = predictions.get(particle, 1.5)
        obs = results[particle]['optimal_width']
        error = abs(pred - obs)
        total_error += error
        print(f"{particle:<15} {pred:<12.1f} {obs:<12.1f} {error:<10.1f}")
    
    avg_error = total_error / len(particles)
    
    print("\n" + "-"*80)
    if correlation < -0.7 and p_value < 0.1:
        print("‚úÖ STRONG CONFIRMATION: Temporal coherence hypothesis VALIDATED")
    elif correlation < -0.5:
        print("‚ö†Ô∏è  MODERATE SUPPORT: Partial confirmation")
    else:
        print("‚ùå HYPOTHESIS REJECTED: No clear relationship")
    
    return correlation, p_value, avg_error

# ================================================================================
# MAIN
# ================================================================================

if __name__ == "__main__":
    print("\nüöÄ STARTING TEMPORAL COHERENCE EXPERIMENT\n")
    
    start_time = time.time()
    
    try:
        results = run_temporal_coherence_experiment()
        
        # Check if we have enough data
        if len(results) >= 3:
            correlation, p_value, avg_error = analyze_temporal_structure(results)
            
            # Save final results
            summary = {
                'results': results,
                'correlation': float(correlation),
                'p_value': float(p_value),
                'avg_error': float(avg_error)
            }
            
            with open('temporal_coherence_discovery.json', 'w') as f:
                json.dump(summary, f, indent=2)
            
            print(f"\n‚úì Saved: temporal_coherence_discovery.json")
        else:
            print(f"\n‚ö†Ô∏è  Only {len(results)}/4 particles completed")
            print("   Check checkpoint file for partial results")
        
        elapsed = time.time() - start_time
        print(f"\n‚è±Ô∏è  Total time: {elapsed/60:.1f} minutes")
        print("="*80 + "\n")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrupted by user")
        print("   Partial results saved in checkpoint file")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
